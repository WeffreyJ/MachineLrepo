# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GR4YZhjeEGKOcqC5Bn6JJE-bGbQrgmCE
"""

!pip install datasets
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_scheduler
import tensorflow_datasets as tfds
import shutil

# Load the IMDB dataset using TensorFlow Datasets
train_data, validation_data = tfds.load(
    'imdb_reviews',
    split=['train[:80%]', 'train[20%:]'],
    as_supervised=True
)

# Convert TensorFlow dataset to PyTorch format
class IMDBDataset(Dataset):
    def __init__(self, tf_dataset, tokenizer, max_length=512):
        self.texts = []
        self.labels = []
        for text, label in tf_dataset:
            self.texts.append(text.numpy().decode('utf-8'))
            self.labels.append(label.numpy())
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        input_text = self.texts[idx]
        label = self.labels[idx]
        inputs = self.tokenizer(input_text, truncation=True, max_length=self.max_length, return_tensors="pt", padding="max_length")
        return {
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize the GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

# Prepare the datasets and dataloaders
train_dataset = IMDBDataset(train_data, tokenizer)
validation_dataset = IMDBDataset(validation_data, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
validation_loader = DataLoader(validation_dataset, batch_size=8, shuffle=False)

# Load the GPT-2 model
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Move the model to the appropriate device (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Fine-tuning setup
optimizer = AdamW(model.parameters(), lr=5e-5)
num_epochs = 3
num_training_steps = num_epochs * len(train_loader)

# Scheduler for learning rate adjustment
scheduler = get_scheduler(
    "linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)

# Fine-tuning loop
for epoch in range(num_epochs):
    model.train()
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)
        loss = outputs.loss

        loss.backward()
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    print(f"Epoch {epoch+1}/{num_epochs} completed. Loss: {loss.item()}")

    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for batch in validation_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)
            val_loss += outputs.loss.item()

    avg_val_loss = val_loss / len(validation_loader)
    print(f"Validation Loss: {avg_val_loss}")

# Save the fine-tuned model
model.save_pretrained('gpt2_imdb_finetuned')
tokenizer.save_pretrained('gpt2_imdb_finetuned')

# Zip the saved model for submission or future use
shutil.make_archive('gpt2_imdb_finetuned', 'zip', 'gpt2_imdb_finetuned')

# Test the model
test_review = "This movie was fantastic! The acting was superb and the plot kept me engaged throughout."
inputs = tokenizer.encode(test_review, return_tensors='pt').to(device)
outputs = model.generate(inputs, max_new_tokens=50)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"Generated Text: {generated_text}")